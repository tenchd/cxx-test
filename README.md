Prototype implementation of streaming spectral sparsifier.

## Compiling and Running on NERSC
Clone the project and `cd` into it.

Run `module load intel` to load the intel mkl code.

Run `export CXX=/usr/bin/g++` which is needed for compilation of C++ code within Rust.

In src/main, the main() function currently calls `lap_test()` which reads the virus data mtx file as a stream of edges, adds them to the sparsifier, and then tries to perform a linear solve on the resulting laplacian (for now, using a right hand side vector generated by multipling the Laplacian matrix by a randomly generated solution vector).

Change the `input_filename` variable to the path to the virus dataset. (It may work even without this change, I can't remember the rules about file permissions on NERSC. I'll check this later)

Run `cargo +nightly run` to run the code. You should see output that indicates the matrix is read from the file, run through the streaming code and collected into a Laplacian matrix. Then the code will attempt to create a trivial rhs vector, and the step where the Laplacian (with about 4.5 million nonzeros) is multiplied by the random solution vetor takes a long time: 10-20 minutes.

The code that performs this mysteriously-slow multiplication is the function `create_trivial_rhs` in src/utils.rs.

You can also run `cargo +nightly bench` to run some simple benchmarking for spmv on matrices of different sizes both in CSC and CSR format. The code for these benchmarks is in src/tests.rs. 

The last two benchmarks (spmv4c and spmv4r) should be close to the conditions of the slow SPMV in the `lap_test()` experiment: This code benchmarks multiplying a 200,000x200,000 matrix with 4,000,000 nonzeros with a randomly generated length 200,000 matrix. It looks like the CSC version only takes around 6 ms, so that suggests I'm doing something wrong in my sparsifier code that seriously degrades performance.
